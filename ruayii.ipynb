{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验目标：**\n",
    "\n",
    "通过本实验，你将深入了解和实践说话人识别技术，并掌握利用声音特征进行有效说话人识别的基本方法，了解不同特征和模型对识别准确率的影响。\n",
    "\n",
    "实验的核心目标是使用TIMIT数据集来训练一个说话人识别系统，涵盖数据预处理、特征提取、模型训练和评估等关键步骤。\n",
    "\n",
    "\n",
    "**实验方法：**\n",
    "\n",
    "**1. 数据预处理和划分(可选)：**\n",
    "  - 数据集下载地址（4月17日前有效）：https://f.ws59.cn/f/du8yd2536vl\n",
    "  - 为了方便大家，我们提供了划分好的TIMIT数据集结构，当然你也可以根据需求自行划分该数据集。\n",
    "  - 为简化难度，我们排除了SA的两个方言句子，并在剩余的8个句子中选取了SX的5个句子和SI的1个句子作为训练集，SI的另外2个句子作为测试集。\n",
    "  - 该链接下载的数据集只保留了音频文件，完整数据集（包含音频对应文本、标注等信息）可参见备注链接下载。\n",
    "  \n",
    "**2. 特征提取：**\n",
    "  - 学习并实现包括但不限于MFCC特征等特征的提取，探索声音信号的频率和时间特性。\n",
    "  - 鼓励尝试和比较其他特征提取方法，例如LPCC或声谱图特征，以理解不同特征对识别性能的影响。\n",
    "  \n",
    "**3. 模型选择和训练：**\n",
    "  - 探索并选择适合的分类器和模型进行说话人识别，如GMM、Softmax分类器或深度学习模型。\n",
    "  - 实现模型训练流程，使用训练集数据训练模型。\n",
    "  \n",
    "**4. 评估和分析：**\n",
    "  - 使用准确率作为主要的评价指标在测试集上评估模型性能。\n",
    "  - 对比不同特征和模型的性能，分析其对说话人识别准确率的影响。\n",
    "  - 可视化不同模型的识别结果和错误率，讨论可能的改进方法。\n",
    "\n",
    "**实验要求：**\n",
    "  - 1.选择并实现至少一种特征的提取，并鼓励尝试其他特征提取方法。\n",
    "  - 2.选择并实现至少一种分类器或模型进行说话人识别，并使用准确率评估指标评估其性能。\n",
    "  - 3.通过实验对比、分析和可视化，撰写详细的实验报告，包括实验目的、实验方法、结果分析和结论。\n",
    "  - 4.实验报告应以清晰、逻辑性强的形式呈现，图表和结果应清楚明了。\n",
    "\n",
    "**其他说明：**\n",
    "  - 实验的最终打分环节会看识别性能，会对原理和实现代码部分做抽查提问，综合评定成绩。\n",
    "  - 我们**鼓励做原创性探索**，即使性能不是很好，但有创新性、有价值、有意义的探索和尝试会有额外加分。\n",
    "  - 原数完整据集下载地址：https://drive.google.com/file/d/180mSIiXN9RVDV2Xn1xcWNkMRm5J5MjN4/view?usp=sharing \\\n",
    "    或国内访问地址（4月17日前有效）：https://f.ws59.cn/f/du8xu130kba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import os\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 可以根据需要导入其他库，比如librosa用于音频处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理(加载数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Dataset/TRAIN\\\\DR1\\\\FCJF0\\\\merge_result.wav', 'CJF0', 'FCJF0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 请从如下地址下载数据集（4月17日前有效）：https://f.ws59.cn/f/du8yd2536vl\n",
    "# 数据集基本信息如下\n",
    "# 方言地区：DR1～DR8\n",
    "# 性别：F/M\n",
    "# 说话者ID：3大写字母+1阿拉伯数字\n",
    "# 句子ID：句子类型（SA/SI/SX）+编号\n",
    "# 详细介绍参见 https://blog.csdn.net/qq_39373179/article/details/103788208\n",
    "#添加路径\n",
    "# 上述链接下载的数据集已经\n",
    "TrainDir = \"Dataset/TRAIN\"\n",
    "TestDir = \"Dataset/TEST\"\n",
    "\n",
    "## 请在这里写代码加载我们划分好的TIMIT训练集和测试集。或者原始完整版数据集。\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class trainset(Dataset):\n",
    "    def __init__(self,dir):\n",
    "        self.dir_path=dir\n",
    "        self.drs=os.listdir(self.dir_path)\n",
    "        self.wavs=dict() # wav文件名：标注\n",
    "        \n",
    "        for dr in self.drs:\n",
    "            speakers=os.listdir(os.path.join(self.dir_path,dr))\n",
    "            for s in speakers:\n",
    "                wavs=os.listdir(os.path.join(self.dir_path,dr,s))\n",
    "                for w in wavs:\n",
    "                    if w[-3:]=='wav': self.wavs[os.path.join(self.dir_path,dr,s,w)]=s[1:]\n",
    "    def __getitem__(self, index):\n",
    "        #第index的键和值\n",
    "        wav=list(self.wavs.keys())[index]\n",
    "        label=self.wavs[wav]\n",
    "        dir_=wav.split('\\\\')[2]\n",
    "        return wav,label,dir_\n",
    "    def __len__(self):\n",
    "        #return len(self.images)\n",
    "        return len(self.wavs)\n",
    "\n",
    "train_dataset=trainset(TrainDir)\n",
    "validation_dataset=trainset(TestDir)\n",
    "res=train_dataset[0]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请编写或使用库函数提取MFCC等音频特征\n",
    "# 提取特征\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(file_path, mfcc=True, chroma=True, mel=True):\n",
    "    # Load audio file\n",
    "    #print(file_path)\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    if mfcc:\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr) \n",
    "        #print(f\"@@@ mfccs shape:{mfccs.shape}\")\n",
    "        features.append(np.mean(mfccs, axis=1)) #20 * t\n",
    "    if chroma:\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        #print(f\"@@@ chroma shape:{chroma.shape}\")\n",
    "        features.append(np.mean(chroma, axis=1)) #12 * t\n",
    "\n",
    "    if mel:\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        #print(f\"@@@ mel_spectrogram shape:{mel_spectrogram.shape}\")\n",
    "        features.append(np.mean(mel_spectrogram, axis=1)) #128 *t\n",
    "    \n",
    "    return np.concatenate(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            inputs, labels, _ = data\n",
    "            #print(inputs)\n",
    "            inputs = torch.tensor([extract_features(file) for file in inputs]).to(device)  # 提取特征并转换为张量\n",
    "            labels_=[]\n",
    "            is_break=False\n",
    "            for label in labels:\n",
    "                if label in label_map:\n",
    "                    labels_.append(label_map[label])\n",
    "                else:\n",
    "                    #print(\"@\")\n",
    "                    is_break=True\n",
    "            if is_break: continue\n",
    "            labels=labels_\n",
    "            #labels = [label_map[label] for label in labels]  # 将人名转换为整数标签\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_size\n",
    "            #print(predicted,labels)\n",
    "            labels=torch.tensor(labels).to(device)\n",
    "            #print(sum(predicted==labels).item())\n",
    "            print(labels) \n",
    "            print(predicted)\n",
    "            correct+=sum(predicted==labels).item()\n",
    "            \n",
    "            #correct += sum(predicted == labels)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy on validation/test set: {:.2f}%'.format(100 * accuracy))\n",
    "    return accuracy*100\n",
    "#evaluate_model(model, validation_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     63\u001b[0m     inputs, labels, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 64\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([extract_features(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m inputs])\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 提取特征并转换为张量\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m       \u001b[38;5;66;03m# 将人名转换为整数标签\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     63\u001b[0m     inputs, labels, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 64\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m inputs])\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 提取特征并转换为张量\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label_map[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m       \u001b[38;5;66;03m# 将人名转换为整数标签\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path, mfcc, chroma, mel)\u001b[0m\n\u001b[0;32m     20\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(chroma, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#12 * t\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel:\n\u001b[1;32m---> 23\u001b[0m     mel_spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#print(f\"@@@ mel_spectrogram shape:{mel_spectrogram.shape}\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(mel_spectrogram, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#128 *t\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HenryLi\\.conda\\envs\\SP\\lib\\site-packages\\librosa\\feature\\spectral.py:2143\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[0;32m   2131\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2132\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2139\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[0;32m   2140\u001b[0m )\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m-> 2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[1;32mc:\\Users\\HenryLi\\.conda\\envs\\SP\\lib\\site-packages\\librosa\\filters.py:239\u001b[0m, in \u001b[0;36mmel\u001b[1;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[0;32m    236\u001b[0m     upper \u001b[38;5;241m=\u001b[39m ramps[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m fdiff[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# .. then intersect them with each other and zero\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     weights[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(norm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslaney\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义DNN模型\n",
    "class SpeakerRecognitionDNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SpeakerRecognitionDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# 设置模型参数和超参数\n",
    "input_size = 160  # 特征向量的大小\n",
    "num_classes = len(set(train_dataset.wavs.values()))  # 类别数，根据数据集的说话人数量确定\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# 初始化模型\n",
    "model = SpeakerRecognitionDNN(input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 准备数据加载器\n",
    "# 创建人名到整数标签的映射\n",
    "label_map = {label: idx for idx, label in enumerate(set(train_dataset.wavs.values()))}\n",
    "\n",
    "# 准备数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "is_from_pth=False\n",
    "if is_from_pth:\n",
    "    path_=\"model_epoch_107.pth\"\n",
    "    model=load_model(model,path_ )\n",
    "    t_e=int(path_.split('_')[-1].split('.')[0])\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    if is_from_pth:\n",
    "        epoch_=epoch+t_e\n",
    "    else: epoch_=epoch\n",
    "    running_loss = 0.0\n",
    "    #evaluate_model(model, validation_loader)\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels, _ = data\n",
    "        inputs = torch.tensor([extract_features(file) for file in inputs]).to(device)  # 提取特征并转换为张量\n",
    "        labels = torch.tensor([label_map[label] for label in labels]).to(device)\n",
    "          # 将人名转换为整数标签\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 10 == 0:  # 每10个mini-batch打印一次loss\n",
    "            print(f'Epoch [{epoch_+1}/{num_epochs+epoch_}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/10}')\n",
    "            running_loss = 0.0\n",
    "    #total += labels.size(0)\n",
    "    # 在每轮训练之后进行评估\n",
    "    print(\"Evaluation after epoch\", epoch_ + 1)\n",
    "    acc_= evaluate_model(model, validation_loader)  # 假设validation_dataset是验证集的数据加载器\n",
    "    #写入log- acc\n",
    "    with open('log.txt', 'a') as f:\n",
    "        f.write(f\"Epoch {epoch_+1}, Accuracy: {acc_} %\\n\")\n",
    "    # 保存模型\n",
    "    if (epoch_+1) %5==0:\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch_+1}.pth\")\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评价指标(准确率Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_epoch_100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m----> 6\u001b[0m test_\u001b[38;5;241m=\u001b[39m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_epoch_100.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m evaluate_model(test_, validation_loader)\n",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, model_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model, model_path):\n\u001b[1;32m----> 3\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\HenryLi\\.conda\\envs\\SP\\lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\HenryLi\\.conda\\envs\\SP\\lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\HenryLi\\.conda\\envs\\SP\\lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_epoch_100.pth'"
     ]
    }
   ],
   "source": [
    "## 请编写代码或使用库函数accuracy_score计算测试集上的准确率Accuracy\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "test_=load_model(model, \"model_epoch_100.pth\")\n",
    "evaluate_model(test_, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  6. 分析和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请使用matplotlib等可视化库对你的实验结果进行可视化分析。\n",
    "## 包括但不限于准确率的对比、错误分类的分析、特征的影响等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 结果讨论\n",
    "讨论你的模型性能，尝试解释为什么某些模型比其他模型表现好，以及可能的改进方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存模型（可选）\n",
    "如果需要，可以在这里添加代码保存你的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
