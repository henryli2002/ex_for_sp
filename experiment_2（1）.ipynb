{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验目标：**\n",
    "\n",
    "通过本实验，你将深入了解和实践说话人识别技术，并掌握利用声音特征进行有效说话人识别的基本方法，了解不同特征和模型对识别准确率的影响。\n",
    "\n",
    "实验的核心目标是使用TIMIT数据集来训练一个说话人识别系统，涵盖数据预处理、特征提取、模型训练和评估等关键步骤。\n",
    "\n",
    "\n",
    "**实验方法：**\n",
    "\n",
    "**1. 数据预处理和划分(可选)：**\n",
    "  - 数据集下载地址（4月17日前有效）：https://f.ws59.cn/f/du8yd2536vl\n",
    "  - 为了方便大家，我们提供了划分好的TIMIT数据集结构，当然你也可以根据需求自行划分该数据集。\n",
    "  - 为简化难度，我们排除了SA的两个方言句子，并在剩余的8个句子中选取了SX的5个句子和SI的1个句子作为训练集，SI的另外2个句子作为测试集。\n",
    "  - 该链接下载的数据集只保留了音频文件，完整数据集（包含音频对应文本、标注等信息）可参见备注链接下载。\n",
    "  \n",
    "**2. 特征提取：**\n",
    "  - 学习并实现包括但不限于MFCC特征等特征的提取，探索声音信号的频率和时间特性。\n",
    "  - 鼓励尝试和比较其他特征提取方法，例如LPCC或声谱图特征，以理解不同特征对识别性能的影响。\n",
    "  \n",
    "**3. 模型选择和训练：**\n",
    "  - 探索并选择适合的分类器和模型进行说话人识别，如GMM、Softmax分类器或深度学习模型。\n",
    "  - 实现模型训练流程，使用训练集数据训练模型。\n",
    "  \n",
    "**4. 评估和分析：**\n",
    "  - 使用准确率作为主要的评价指标在测试集上评估模型性能。\n",
    "  - 对比不同特征和模型的性能，分析其对说话人识别准确率的影响。\n",
    "  - 可视化不同模型的识别结果和错误率，讨论可能的改进方法。\n",
    "\n",
    "**实验要求：**\n",
    "  - 1.选择并实现至少一种特征的提取，并鼓励尝试其他特征提取方法。\n",
    "  - 2.选择并实现至少一种分类器或模型进行说话人识别，并使用准确率评估指标评估其性能。\n",
    "  - 3.通过实验对比、分析和可视化，撰写详细的实验报告，包括实验目的、实验方法、结果分析和结论。\n",
    "  - 4.实验报告应以清晰、逻辑性强的形式呈现，图表和结果应清楚明了。\n",
    "\n",
    "**其他说明：**\n",
    "  - 实验的最终打分环节会看识别性能，会对原理和实现代码部分做抽查提问，综合评定成绩。\n",
    "  - 我们**鼓励做原创性探索**，即使性能不是很好，但有创新性、有价值、有意义的探索和尝试会有额外加分。\n",
    "  - 原数完整据集下载地址：https://drive.google.com/file/d/180mSIiXN9RVDV2Xn1xcWNkMRm5J5MjN4/view?usp=sharing \\\n",
    "    或国内访问地址（4月17日前有效）：https://f.ws59.cn/f/du8xu130kba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实验准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入必要的库\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 可以根据需要导入其他库，比如librosa用于音频处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请从如下地址下载数据集（4月17日前有效）：https://f.ws59.cn/f/du8yd2536vl\n",
    "# 数据集基本信息如下\n",
    "# 方言地区：DR1～DR8\n",
    "# 性别：F/M\n",
    "# 说话者ID：3大写字母+1阿拉伯数字\n",
    "# 句子ID：句子类型（SA/SI/SX）+编号\n",
    "# 详细介绍参见 https://blog.csdn.net/qq_39373179/article/details/103788208\n",
    "\n",
    "# 上述链接下载的数据集已经\n",
    "TrainDir = \"Dataset/TRAIN\"\n",
    "TestDir = \"Dataset/TEST\"\n",
    "## 请在这里写代码加载我们划分好的TIMIT训练集和测试集。或者原始完整版数据集。\n",
    "\n",
    "\n",
    "def pad_or_truncate(array, max_length):\n",
    "    \"\"\";\"\"\"\n",
    "    if array.shape[1] < max_length:\n",
    "        pad_size = max_length - array.shape[1]\n",
    "        array = np.pad(\n",
    "            array, ((0, 0), (0, pad_size)), mode=\"constant\", constant_values=(0, 0)\n",
    "        )\n",
    "    else:\n",
    "        array = array[:, :max_length]\n",
    "    return array\n",
    "\n",
    "\n",
    "class trainset(Dataset):\n",
    "    def __init__(self, dir, mode=0):\n",
    "        self.mode = mode\n",
    "        self.dir_path = dir\n",
    "        self.drs = os.listdir(self.dir_path)\n",
    "        self.wavs = dict()  # wav 文件名：标注\n",
    "        self.label_to_index = {}  # 字典存储标签和索引\n",
    "        self.current_index = 0\n",
    "\n",
    "        # 初始化字典并填充 wav 列表\n",
    "        for dr in self.drs:\n",
    "            speakers = os.listdir(os.path.join(self.dir_path, dr))\n",
    "            for s in speakers:\n",
    "                wavs = os.listdir(os.path.join(self.dir_path, dr, s))\n",
    "                for w in wavs:\n",
    "                    if w.split(\".\")[-1] == \"WAV\":\n",
    "                        label = dr + s[:1]  # 创建标签\n",
    "                        if label not in self.label_to_index:\n",
    "                            self.label_to_index[label] = self.current_index\n",
    "                            self.current_index += 1\n",
    "                        self.wavs[os.path.join(self.dir_path, dr, s, w)] = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav = list(self.wavs.keys())[index]\n",
    "        label = self.wavs[wav]\n",
    "        # 获取标签对应的索引\n",
    "        label_index = self.label_to_index[label]\n",
    "        # 将标签索引转换为张量\n",
    "        label_tensor = torch.tensor(label_index, dtype=torch.long).to(device)\n",
    "        waveform, sample_rate = torchaudio.load(wav)\n",
    "\n",
    "        if self.mode == 0:\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=waveform.numpy()[0], sr=sample_rate, n_mfcc=13\n",
    "            )\n",
    "            mfcc_db = librosa.power_to_db(mfcc, ref=np.max)\n",
    "            max_length = 128  # 这个值可以根据数据集中最长的样本来设置\n",
    "            mfcc_db = pad_or_truncate(mfcc_db, max_length)\n",
    "\n",
    "            mfcc_db = torch.tensor(mfcc).float().to(device)\n",
    "            return mfcc_db, label_tensor\n",
    "        else:\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=waveform.numpy()[0], sr=sample_rate\n",
    "            )\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            max_length = 128  # 这个值可以根据数据集中最长的样本来设置\n",
    "            mel_spec_db = pad_or_truncate(mel_spec_db, max_length)\n",
    "\n",
    "            mel_spec_db = torch.tensor(mel_spec_db).float().to(device)\n",
    "\n",
    "            return mel_spec_db, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "\n",
    "    def target(self):\n",
    "        return len(self.label_to_index)\n",
    "\n",
    "\n",
    "train_dataset = trainset(TrainDir, 1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_dataset = trainset(TestDir, 1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "target = train_dataset.target()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请编写或使用库函数提取MFCC等音频特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型选择和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 47/47 [01:51<00:00,  2.37s/it, loss=2.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.7009382247924805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]:  81%|████████  | 38/47 [00:35<00:08,  1.09it/s, loss=2.66]"
     ]
    }
   ],
   "source": [
    "## 在这部分，你可以选择不同的分类器和模型如GMM模型来进行实验\n",
    "class AudioLSTM(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(AudioLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=13, hidden_size=128, num_layers=2, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(128, out_features=target)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "        output_size = 28800  # Adjusted calculation\n",
    "        self.fc1 = nn.Linear(output_size, 100)\n",
    "        self.fc2 = nn.Linear(100, target)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Adds a channel dimension\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = AudioCNN(target=target).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        for mfccs, labels in loop:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mfccs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "train_model(10)\n",
    "\n",
    "torch.save(model.state_dict(), \"model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 评价指标(准确率Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请编写代码或使用库函数accuracy_score计算测试集上的准确率Accuracy\n",
    "\n",
    "model = AudioCNN(target=target)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pth\")).to(device)\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mfccs, labels in test_loader:\n",
    "            outputs = model(mfccs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算正确的数量\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # 计算平均损失和准确度\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  6. 分析和可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 请使用matplotlib等可视化库对你的实验结果进行可视化分析。\n",
    "## 包括但不限于准确率的对比、错误分类的分析、特征的影响等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 结果讨论\n",
    "讨论你的模型性能，尝试解释为什么某些模型比其他模型表现好，以及可能的改进方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存模型（可选）\n",
    "如果需要，可以在这里添加代码保存你的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
